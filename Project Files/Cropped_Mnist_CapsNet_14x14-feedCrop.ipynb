{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-55c8ffe44dcc>:15: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "np.random.seed(40)\n",
    "tf.set_random_seed(40)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "cDim = 14\n",
    "cDimS = cDim**2\n",
    "\n",
    "def feedCrop(images,labels):\n",
    "    croppedImages = images;\n",
    "    croppedBatch = np.empty(shape=(len(images),cDimS), dtype=np.float64)\n",
    "    for x in range(len(croppedImages)):\n",
    "        currentImage = croppedImages[x].reshape(28,28)\n",
    "        newImage = np.empty(shape=(cDim,cDim),dtype=np.float64);\n",
    "        \n",
    "        l = random.randint(5,10)\n",
    "        w = random.randint(5,10)\n",
    "\n",
    "        newImage = currentImage[l:l+cDim,w:w+cDim] \n",
    "        \n",
    "        croppedBatch[x] = newImage.reshape(cDimS);\n",
    "        \n",
    "    return croppedBatch,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "batch_size = 5\n",
    "train1, labels1 = mnist.train.next_batch(batch_size)\n",
    "train1, labels1 = feedCrop(train1,labels1)\n",
    "\n",
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = train1[index].reshape(cDim, cDim)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-b9fced495adb>:6: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-4-b9fced495adb>:106: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "#Squashing function ----------------------------------------------\n",
    "\n",
    "def squash(s, axis=-1, epsilon=1e-7, name=None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector\n",
    "\n",
    "#Safe norm function ---------------------------------------------\n",
    "\n",
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)\n",
    "\n",
    "#Architecture:---------------------------------------------------- adapted from hanson-ml extra_capsnets.py\n",
    "\n",
    "#input layer placeholder\n",
    "X = tf.placeholder(shape=[None, cDim, cDim, 1], dtype=tf.float32, name=\"X\")\n",
    "\n",
    "#Convolutional parameters\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 4 * 4  # 1152 primary capsules\n",
    "caps1_n_dims = 8\n",
    "\n",
    "#Convolutional layer creation\n",
    "conv1_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\":3,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu6,\n",
    "}\n",
    "conv2_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\":3,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu6,\n",
    "}\n",
    "conv3_params = {\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\":3,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu6,\n",
    "}\n",
    "conv4_params = {\n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 5,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu6,\n",
    "}\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, name=\"conv1\", **conv1_params)\n",
    "conv2 = tf.layers.conv2d(conv1, name=\"conv2\", **conv2_params)\n",
    "conv3 = tf.layers.conv2d(conv2, name=\"conv3\", **conv3_params)\n",
    "conv4 = tf.layers.conv2d(conv3, name=\"conv4\", **conv4_params)\n",
    "\n",
    "\n",
    "#reshape of the concolutional layer to the primary capsule dimensions\n",
    "caps1_raw = tf.reshape(conv4, [-1, caps1_n_caps, caps1_n_dims],\n",
    "                       name=\"caps1_raw\")\n",
    "#Squashes the raw layer from 0 to 1.\n",
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")\n",
    "\n",
    "#digit capsule (secondary) layer\n",
    "caps2_n_caps = 10\n",
    "caps2_n_dims = 32\n",
    "\n",
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.truncated_normal(\n",
    "    shape=(1, caps1_n_caps, caps2_n_caps, caps2_n_dims, caps1_n_dims),\n",
    "    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "W = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "#okay.\n",
    "batch_size = tf.shape(X)[0]\n",
    "\n",
    "#tiled weights over batch size?\n",
    "W_tiled = tf.tile(W, [batch_size, 1, 1, 1, 1], name=\"W_tiled\")\n",
    "\n",
    "#science\n",
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "caps1_output_tile = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "caps1_output_tiled = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")\n",
    "\n",
    "#gets Uj|i\n",
    "caps2_predicted = tf.matmul(W_tiled, caps1_output_tiled,\n",
    "                            name=\"caps2_predicted\")\n",
    "#Routing ------------------------------------------------------------------\n",
    "#routing round 1\n",
    "\n",
    "raw_weights = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1],\n",
    "                       dtype=np.float32, name=\"raw_weights\")\n",
    "\n",
    "routing_weights = tf.nn.softmax(raw_weights, dim=2, name=\"routing_weights\")\n",
    "\n",
    "weighted_predictions = tf.multiply(routing_weights, caps2_predicted,\n",
    "                                   name=\"weighted_predictions\")\n",
    "weighted_sum = tf.reduce_sum(weighted_predictions, axis=1, keep_dims=True,\n",
    "                             name=\"weighted_sum\")\n",
    "\n",
    "#squash output\n",
    "caps2_output_round_1 = squash(weighted_sum, axis=-2,\n",
    "                              name=\"caps2_output_round_1\")\n",
    "\n",
    "caps2_output_round_1_tiled = tf.tile(\n",
    "    caps2_output_round_1, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_1_tiled\")\n",
    "\n",
    "agreement = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement\")\n",
    "\n",
    "#routing round 2\n",
    "raw_weights_round_2 = tf.add(raw_weights, agreement,\n",
    "                             name=\"raw_weights_round_2\")\n",
    "\n",
    "routing_weights_round_2 = tf.nn.softmax(raw_weights_round_2,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_2\")\n",
    "weighted_predictions_round_2 = tf.multiply(routing_weights_round_2,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_2\")\n",
    "weighted_sum_round_2 = tf.reduce_sum(weighted_predictions_round_2,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_2\")\n",
    "\n",
    "#squash output\n",
    "caps2_output_round_2 = squash(weighted_sum_round_2,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_2\")\n",
    "\n",
    "caps2_output_round_2_tiled = tf.tile(\n",
    "    caps2_output_round_2, [1, caps1_n_caps, 1, 1, 1],\n",
    "    name=\"caps2_output_round_2_tiled\")\n",
    "\n",
    "agreement2 = tf.matmul(caps2_predicted, caps2_output_round_1_tiled,\n",
    "                      transpose_a=True, name=\"agreement\")\n",
    "\n",
    "#routing round 2\n",
    "raw_weights_round_3 = tf.add(raw_weights, agreement2,\n",
    "                             name=\"raw_weights_round_3\")\n",
    "\n",
    "routing_weights_round_3 = tf.nn.softmax(raw_weights_round_3,\n",
    "                                        dim=2,\n",
    "                                        name=\"routing_weights_round_3\")\n",
    "weighted_predictions_round_3 = tf.multiply(routing_weights_round_3,\n",
    "                                           caps2_predicted,\n",
    "                                           name=\"weighted_predictions_round_3\")\n",
    "weighted_sum_round_3 = tf.reduce_sum(weighted_predictions_round_3,\n",
    "                                     axis=1, keep_dims=True,\n",
    "                                     name=\"weighted_sum_round_3\")\n",
    "\n",
    "#squash output\n",
    "caps2_output_round_3 = squash(weighted_sum_round_3,\n",
    "                              axis=-2,\n",
    "                              name=\"caps2_output_round_3\")\n",
    "\n",
    "#final output for now.-----------------------------------------------------\n",
    "caps2_output = caps2_output_round_3\n",
    "\n",
    "\n",
    "#Loss ---------------------------------------------------------------------\n",
    "\n",
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")\n",
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")\n",
    "\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")\n",
    "\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5\n",
    "\n",
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
    "                              name=\"caps2_output_norm\")\n",
    "\n",
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 10),\n",
    "                           name=\"present_error\")\n",
    "\n",
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 10),\n",
    "                          name=\"absent_error\")\n",
    "\n",
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "           name=\"L\")\n",
    "\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "\n",
    "#Reconstruction -----------------------------------------------------------\n",
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                               name=\"mask_with_labels\")\n",
    "\n",
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")\n",
    "\n",
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")\n",
    "\n",
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "    name=\"reconstruction_mask_reshaped\")\n",
    "\n",
    "\n",
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")\n",
    "\n",
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")\n",
    "\n",
    "#Decoder ----------------------------------------------------------------------\n",
    "\n",
    "n_hidden1 = 1024\n",
    "n_hidden2 = 2048\n",
    "n_output = cDim * cDim\n",
    "\n",
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")\n",
    "\n",
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")\n",
    "\n",
    "alpha = 0.0001\n",
    "\n",
    "\n",
    "#Final loss ----------------------------------------------------------------------\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")\n",
    "\n",
    "\n",
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training ------------------------------------------------------------------------\n",
    "n_epochs = 25\n",
    "batch_size = 100\n",
    "restore_checkpoint = True\n",
    "\n",
    "n_iterations_per_epoch = mnist.train.num_examples // batch_size\n",
    "n_iterations_validation = mnist.validation.num_examples // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./CMnistCaps_14x14_feedCrop_A12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 91.0400%  Loss: 0.083914 (improved)\n",
      "Epoch: 2  Val accuracy: 93.8600%  Loss: 0.055127 (improved)\n",
      "Epoch: 3  Val accuracy: 94.0000%  Loss: 0.051254 (improved)\n",
      "Epoch: 4  Val accuracy: 94.8400%  Loss: 0.044770 (improved)\n",
      "Epoch: 5  Val accuracy: 95.0600%  Loss: 0.042936 (improved)\n",
      "Epoch: 6  Val accuracy: 95.8800%  Loss: 0.036859 (improved)\n",
      "Epoch: 7  Val accuracy: 95.4200%  Loss: 0.040274\n",
      "Epoch: 8  Val accuracy: 95.4600%  Loss: 0.038661\n",
      "Epoch: 9  Val accuracy: 95.9600%  Loss: 0.035350 (improved)\n",
      "Epoch: 10  Val accuracy: 96.1600%  Loss: 0.034062 (improved)\n",
      "Epoch: 11  Val accuracy: 95.7200%  Loss: 0.034898\n",
      "Epoch: 12  Val accuracy: 96.2000%  Loss: 0.033985 (improved)\n",
      "Epoch: 13  Val accuracy: 96.3600%  Loss: 0.032761 (improved)\n",
      "Epoch: 14  Val accuracy: 95.7200%  Loss: 0.033194\n",
      "Epoch: 15  Val accuracy: 96.2400%  Loss: 0.032724 (improved)\n",
      "Epoch: 16  Val accuracy: 96.1600%  Loss: 0.032141 (improved)\n",
      "Epoch: 17  Val accuracy: 96.2600%  Loss: 0.031817 (improved)\n",
      "Epoch: 18  Val accuracy: 96.6000%  Loss: 0.030065 (improved)\n",
      "Epoch: 19  Val accuracy: 96.5800%  Loss: 0.029612 (improved)\n",
      "Epoch: 20  Val accuracy: 95.8600%  Loss: 0.033555\n",
      "Epoch: 21  Val accuracy: 95.8400%  Loss: 0.035039\n",
      "Epoch: 22  Val accuracy: 96.7000%  Loss: 0.029276 (improved)\n",
      "Epoch: 23  Val accuracy: 96.3800%  Loss: 0.028305 (improved)\n",
      "Epoch: 24  Val accuracy: 96.2200%  Loss: 0.031326\n",
      "Epoch: 25  Val accuracy: 96.3600%  Loss: 0.031342\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    lossFail = 0;\n",
    "    for epoch in range(0,n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch, y_batch = feedCrop(X_batch,y_batch)\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, cDim, cDim, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "            X_batch, y_batch = feedCrop(X_batch,y_batch)\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, cDim, cDim, 1]),\n",
    "                               y: y_batch})\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val\n",
    "            lossFail = 0;\n",
    "        else:\n",
    "            lossFail += 1;\n",
    "            if(lossFail == 5):\n",
    "                break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CMnistCaps_14x14_feedCrop_A12\n",
      "Epoch: 26  Val accuracy: 96.3200%  Loss: 0.029455\n",
      "Epoch: 27  Val accuracy: 95.9600%  Loss: 0.031700\n",
      "Epoch: 28  Val accuracy: 96.6800%  Loss: 0.027941 (improved)\n",
      "Epoch: 29  Val accuracy: 96.4600%  Loss: 0.029311\n",
      "Epoch: 30  Val accuracy: 96.9000%  Loss: 0.025053 (improved)\n",
      "Epoch: 31  Val accuracy: 96.4400%  Loss: 0.030185\n",
      "Epoch: 32  Val accuracy: 96.5200%  Loss: 0.028051\n",
      "Epoch: 33  Val accuracy: 96.6000%  Loss: 0.028287\n",
      "Epoch: 34  Val accuracy: 96.7000%  Loss: 0.027792\n",
      "Iteration: 453/550 (82.4%)  Loss: 0.00976"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "    lossFail = 0;\n",
    "    for epoch in range(25,51):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch, y_batch = feedCrop(X_batch,y_batch)\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, cDim, cDim, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "            X_batch, y_batch = feedCrop(X_batch,y_batch)\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, cDim, cDim, 1]),\n",
    "                               y: y_batch})\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val\n",
    "            lossFail = 0;\n",
    "        else:\n",
    "            lossFail += 1;\n",
    "            if(lossFail == 5):\n",
    "                break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CMnistCaps_14x14_feedCrop_A12\n",
      "Final test accuracy: 96.2500%  Loss: 0.030448   \n"
     ]
    }
   ],
   "source": [
    "n_iterations_test = mnist.test.num_examples // batch_size\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "        X_batch, y_batch = feedCrop(X_batch,y_batch)\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, cDim, cDim, 1]),\n",
    "                           y: y_batch})\n",
    "        \n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        \n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CMnistCaps_14x14_feedCrop_A12\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "labs = np.empty(shape = (n_samples))\n",
    "\n",
    "sample_images, labs = feedCrop(mnist.test.images[:n_samples],labs)\n",
    "sample_images = sample_images.reshape([-1, cDim, cDim, 1])\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    caps2_output_value, decoder_output_value, y_pred_value = sess.run(\n",
    "            [caps2_output, decoder_output, y_pred],\n",
    "            feed_dict={X: sample_images,\n",
    "                       y: np.array([], dtype=np.int64)})\n",
    "\n",
    "sample_images = sample_images.reshape(-1, cDim, cDim)\n",
    "reconstructions = decoder_output_value.reshape([-1, cDim, cDim])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(mnist.test.labels[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.title(\"Predicted:\" + str(y_pred_value[index]))\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6961604\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "        \n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        variable_parameters *= dim.value\n",
    "   \n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
